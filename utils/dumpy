#!/usr/bin/env python3

"""
@desc:  Tool to handle big endian binary files: split, write to ascii, print to stdout, etc.
@auth:  Jayghosh S. Rao
@usage: ./dumpy <FILES> {-w outfile | -p} {-s <number of splits>} {-v <d | i>} {-z} {-d}
@help: ./dumpy -h
@examples:
    $ dumpy mrng -v i -p -n 4 | vipe
    $ dumpy two-spheres.ascii -v f -e '<' -w two-spheres.xyzd -a
    $ dumpy two-spheres.xyzd0 -v f -e '<' -p
    $ dumpy --packing packing.xyzd --nfo packing.nfo -w fixed.xyzd
    $ dumpy dummy --fill <value> <n_nodes> -n <ndf> -w dummy -v <vartype>
    $ dumpy data.all --extract-slice data.test.last --nrows 4762 --ndf 2 --skip 25

Alternatives: od (octaldump) to convert from bin->ascii
    $ od -A n -t fD --endian=big -w24 mxyz
"""

## TODO: Test extract_slice on large files (will there be overflows?)
## TODO: Print nth column
## TODO: Try out np.memmap() seen here https://stackoverflow.com/questions/5854515/interactive-large-plot-with-20-million-sample-points-and-gigabytes-of-data

from __future__ import print_function

import argparse
import struct
import sys
import itertools
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

def find_up(name, startdir='.', stopdir='~', multiple=False, norecurse=False, nobreak=False):
    path = Path(startdir).resolve()                   ## start searching in startdir
    stop = Path(stopdir).expanduser().parent.resolve()   ## search in the stopdir, but not beyond. stopdir is inclusive
    # print('Searching recursively until', Path(stopdir).expanduser() )
    res_nobreak = []
    res=[]
    while path != stop:
        if norecurse:
            res = [ x for x in path.glob(name) ]
        else:
            res = [ x for x in path.rglob(name) ]
        if nobreak:
            res_nobreak.extend(res)
        else:
            if res:
                if multiple:
                    return res
                else:
                    return [res[0]]
        path = path.parent
    return list(set(res_nobreak))

def bin_to_arr(filename, dataformat, skip=0, skiprows=0, nrows=0, ncols=0):
    datasize = dataformatsize(dataformat)

    with(open(filename, 'rb')) as input:
        input.seek(skip * nrows * ncols * datasize + skiprows * ncols * datasize, 0)
        myiter = struct.iter_unpack(dataformat, input.read())

        arr = []
        for i in myiter:
            arr.append(i[0])

        return arr

        # arr2 = arr[len(arr)//2:]
        # arr2 = arr[0:len(arr)//2]

def extract_slice(src, dest, dataformat, bufsize, skip=0, skiprows=0, nrows=0, ncols=0):
    """
    Good for slicing/extracting ts from data.all
    """
    datasize = dataformatsize(dataformat)
    length = nrows * ncols * datasize

    print("nrows = {nrows}\nncols = {ncols}".format(nrows=nrows, ncols=ncols))

    with open(src,'rb') as f1:
        f1.seek(skip * nrows * ncols * datasize + skiprows * ncols * datasize, 0)
        with open(dest,'wb') as f2:
            while length:
                chunk = min(bufsize,length)
                data = f1.read(chunk)
                f2.write(data)
                length -= chunk

def dataformatsize(dataformat):
    vartype = dataformat[1]
    datasize = 0
    if vartype == 'd':
        datasize = 8
    elif vartype == 'f':
        datasize = 4
    elif vartype == 'i':
        datasize = 4
    return datasize

def arr_to_bin(arr, filename, dataformat):
    with(open(filename, 'wb')) as output:
        for i in arr:
            output.write(struct.pack(dataformat, i))

def arr_to_bin_unpacked(arr, filename, vartype):
    ## vartype = d, f etc
    ## NOTE: uses native by default endianness
    with(open(filename, 'wb')) as output:
        output.write(struct.pack(str(len(arr)) + vartype, *arr))

def arr_to_ascii(arr, filename):
    with(open(filename, 'w')) as output:
        for item in arr:
            output.write(item)

def ascii_to_arr(filename, vartype):
    with(open(filename, 'r')) as inputfile:
        arr = []
        for line in inputfile:
            val = line.strip()
            arr.append(val)
        if(vartype == 'i'):
            arr = [ int(x) for x in arr]
        elif(vartype == 'f'):
            arr = [ np.float32(x) for x in arr]
        elif(vartype == 'd'):
            arr = [ float(x) for x in arr]
        return arr


def print_data(arr, n, vartype):
    if vartype == 'd':
        for chunk in grouper(arr,n):
            print("\t".join("%.6E" % x for x in chunk))
    elif vartype == 'i':
        for chunk in grouper(arr,n):
            print("\t".join("%d" % x for x in chunk))
    elif vartype == 'f':
        for chunk in grouper(arr,n):
            print("\t".join("%.6E" % x for x in chunk))
    # for item in arr:
    #     print(item)



def grouper(iterable, n):
    it = iter(iterable)
    while True:
       chunk = tuple(itertools.islice(it, n))
       if not chunk:
           return
       yield chunk

def scatterPlot(x, y, z, s=4):
    fig = plt.figure(figsize=(5,5))
    ax1=fig.add_subplot(2,2,1, projection='3d')
    ax2=fig.add_subplot(2,2,2)
    ax3=fig.add_subplot(2,2,3)
    ax4=fig.add_subplot(2,2,4)

    ax1.set(title='3D view')
    ax1.set(xlabel='X')
    ax1.set(ylabel='Y')
    ax1.set(zlabel='Z')
    ax1.scatter(x,y,z,s=s)

    ## TODO: Add corner plot

    for ax,xy,(_x,_y) in zip([ax2, ax3, ax4], ['XZ', 'ZY', 'XY'], [(x,z), (z,y), (x,y)]): 
        ax.set(xlabel=xy[0])
        ax.set(ylabel=xy[1])
        ax.scatter(_x, _y, s=s)
        ax.axes.xaxis.set_ticks([])
        ax.axes.yaxis.set_ticks([])

    fig.savefig('projections.pdf')
    fig.savefig('projections.png', dpi=300)


def calcFinalScalingFactor(filename):
    """
    Newly generated meshes with packinggenerator are out of scale, beads need to be unshrunk.
    """
    data = {}
    with open(filename) as fp:
        for line in fp:
            linesplit = line.strip().split(':')
            data.update({linesplit[0].strip() : linesplit[1].strip().split('(')[0].strip()})
    por_theoretical = float(data['Theoretical Porosity'])
    por_final = float(data['Final Porosity'])

    final_scaling_factor = ( (1-por_final) / (1-por_theoretical) )**(1/3)
    print("Final Scaling Factor:", final_scaling_factor, file=sys.stderr)
    return final_scaling_factor


def mixdInfo(filename, length, nrows, ncols):
    print("Assuming tetrahedrons.")
    nen = 4
    nef = 4
    nsd = 3
    ne = 0
    if 'mien' in filename:
        ne = length / nen
        print("ne: {ne}".format(ne=ne))
    elif 'mxyz' in filename:
        nn = length / nsd
        print("nn: {nn}".format(nn=nn))
    elif 'mrng' in filename:
        nrng = length / nef ## TODO: check if this is right
        print("nrng: {nrng}".format(nrng=nrng))
    elif 'data' in filename:
        nrec = length / (nrows * ncols)
        print("nrec: {nrec}".format(nrec=nrec))


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--plot", action='store_true', help="Plot X Y as scatter plot")
    ap.add_argument("-p", "--print", action='store_true', help="Print to STDOUT")
    ap.add_argument("-a", "--ascii_input", action='store_true', help="input is ascii file")
    ap.add_argument("-v", "--vartype", help="type of variable stored (d | i)", default='d', choices=['d', 'f', 'i'])
    ap.add_argument("-e", "--endianness", help="> or <", default='>')
    ap.add_argument("-n", "--ndf", help="number of items per row, or ncols", default=1, type=int)
    ap.add_argument("-m", "--nrows", help="number of rows. necessary when using --skip or --skiprows", default=0, type=int) ## Default=0 turns off skipping
    ap.add_argument("-s", "--split", help="split file into pieces", default = 1, type=int)
    ap.add_argument("-w", "--write", help="write to bin file")
    ap.add_argument("-so", "--scale-offset", help="Scale every ndf'th number starting from scale-offset. Index starts from 0.", default=0, type=int)
    ap.add_argument("-sf", "--scale-factor", help="Scale every ndf'th number starting from scale-offset.", default=1, type=float)
    ap.add_argument("-z", "--zero-pad", action='store_true', help="Pre-pad file with zeroes")
    ap.add_argument("-d", "--double", action='store_true', help="Double the file. Useful for converting semi-discrete to space-time data, although cat is better.")
    ap.add_argument("-f", "--fill", nargs=2, default = [0, 0], help="Fill the given file with values")
    ap.add_argument("-c", "--compare", help="Compare input files with this reference file.")
    ap.add_argument("-t", "--tolerance", type=float, default=1e-6, help="Tolerance for comparison. Default=1e-6")

    # TODO:
    ap.add_argument("--skip", type=int, default=0, help="Skip <n> steps of the BINARY input file") ## n * rows * cols
    ap.add_argument("--skiprows", type=int, default=0, help="Skip <n> rows of the BINARY input file. Requires specifying --nrows.") ## n * cols
    ap.add_argument("--fastwrite", help="Quick write out to file without storing full data in RAM.")
    ap.add_argument("--extract-slice", help="Quick write out to file without storing full data in RAM.")
    ap.add_argument("--bufsize", help="Read/write buffer size for --extract-slice. Default = nrows * ncols * datasize (the length of 1 step of the input data)", type=int)

    ap.add_argument("--print-col", type=int, help="print nth column. 0-indexed.")

    ap.add_argument("--dpacking", action='store_true', help="Input file is packing.xyzd with e=< and n=4 v=d.")
    ap.add_argument("--fpacking", action='store_true', help="Input file is packing.xyzd with e=< and n=4 v=f.")
    ap.add_argument("--nfo", help="Read newly generated packing data and calculate final scaling factor from NFO file.")
    ap.add_argument("--scatter-size", type=float, default=4,  help="Size of dots on scatter plots generated by --nfo")
    ap.add_argument("--norm", help="Norm order")
    ap.add_argument("--norm-var", help="Calculate Norm for this variable. Index starts at 0.")
    ap.add_argument("-i", "--info", action='store_true', help="Dump info.")
    ap.add_argument("FILES", nargs='*', help="files")
    args = vars(ap.parse_args())

    if args['dpacking']:
        args['ndf'] = 4
        args['endianness'] = '<'
        args['plot'] = True
        args['vartype'] = 'd'

    if args['fpacking']:
        args['ndf'] = 4
        args['endianness'] = '<'
        args['plot'] = True
        args['vartype'] = 'f'

    if args['nfo']:
        # ## TODO: Copy this over to pack.py as well
        final_scaling_factor = calcFinalScalingFactor(args['nfo'])
        # Allows scaling the radius, the 4th component of the xyzd vector
        args['scale_offset'] = 3
        args['scale_factor'] = final_scaling_factor

    vartype = args['vartype']
    ndf = args['ndf']
    endianness = args['endianness']
    dataformat = endianness + vartype
    # datasize = 0

    refarray=[]
    if args['compare']:
        refarray = bin_to_arr(args['compare'], dataformat, skip=args['skip'], skiprows=args['skiprows'], nrows=args['nrows'], ncols=args['ndf'])



    scale_offset= args['scale_offset']
    scale_factor = args['scale_factor']

    infiles = args['FILES']

    # if args['fill']:
    #     args['write'] = args['FILES']

    # split = 1
    # if args['split']:
    #     split = args['split']

    print('Note: Default endianness is big (>) to deal with xns files.', file=sys.stderr)

    for infile in infiles:
        # Input
        print("Infile: ", infile, file=sys.stderr)

        if args['extract_slice']:
            if args['nrows'] == 0:
                ap.error("--extract-slice requires -m/--nrows and -n/--ndf to be set properly.")

            if args['bufsize']:
                bufsize = args['bufsize']
            else:
                bufsize = args['nrows'] * args['ndf'] * dataformatsize(dataformat)

            print("Extracting slice using buffer size:", bufsize)
            extract_slice(infile, args['extract_slice'], dataformat, bufsize, skip=args['skip'], skiprows=args['skiprows'], nrows=args['nrows'], ncols=args['ndf'])

            break
        if args['ascii_input']:
            array = ascii_to_arr(infile, vartype)
        else:
            array = bin_to_arr(infile, dataformat, skip=args['skip'], skiprows=args['skiprows'], nrows=args['nrows'], ncols=args['ndf'])

        if args['compare']:
            result = map(lambda x,y:abs(x-y)<args['tolerance'], array, refarray)
            reslist = list(result)
            if all(reslist):
                print("Similar!")
                sys.exit(0)
            else:
                print("Different!")
                print([i for i,x in enumerate(reslist) if not x])
                sys.exit(-1)

        ## TODO: Clean this up
        ## Apply scale_factor to every ndf'th number
        ## Best for scaling the last variable (step = n), or the entire column (step = 1)
        array[scale_offset::args['ndf']] = [x*scale_factor for x in array[scale_offset::args['ndf']]]

        normDict = {
                '0':0,
                'avg': 0,
                '1':1,
                '2':2,
                'max': np.inf,
                'fro': 'fro'
                }

        if args['norm_var']:
            norm = np.linalg.norm(array[ int(args['norm_var']):: int(args['ndf']) ], ord=normDict[args['norm']])
            print(args['norm'], "norm of var at pos", args['norm_var'], "is", norm)
            if args['norm'] == 'avg':
                asum = sum(array[int(args['norm_var']) :: int(args['ndf']) ])
                alen = len(array[int(args['norm_var']) :: int(args['ndf']) ])
                print("Average of var[%s] over %d values is %f" % (args['norm_var'], alen, asum/alen ) )

        # Modifications
        zero_array = [ 0 for item in array ]
        if args['zero_pad']:
            array = zero_array + array

        if args['double']:
            array = array + array



        if args['fill']:
            if(vartype == 'i'):
                dummy = [ int(args['fill'][0]) ]
            elif(vartype == 'f'):
                dummy = [ float(args['fill'][0]) ]
            elif(vartype == 'd'):
                dummy = [ float(args['fill'][0]) ]
            array.extend( dummy * args['ndf']*int(args['fill'][1]))

        # to split the file into chunks
        out_arrays = []
        out_arrays = np.array_split(np.array(array), args['split'])

        if args['print_col']:
            for arr in out_arrays:
                print_data(np.reshape(arr, (len(arr)//ndf,ndf))[:,args['print_col']], 1, vartype)

        # Output to STDOUT
        if args['print']:
            for arr in out_arrays:
                print_data(arr, ndf, vartype)



        if args['info']:
            for arr in out_arrays:
                print('Type:', type(arr[0]))
                print('Serial Length:', len(arr))
                print('Proper Length:', len(arr) / ndf)
                ## TODO: Can separate this out and fully automate this for all --mixd files
                minffile = find_up("minf")
                minfdict = {}
                with open(minffile[0], 'r') as minf:
                    for line in minf:
                        if line.strip()[0] != '#':
                            minfdict.update({line.strip().split()[0].strip(): int(line.strip().split()[1])})
                if(args['nrows']):
                    mixdInfo(args['FILES'][0], len(arr), args['nrows'], args['ndf'])
                else:
                    mixdInfo(args['FILES'][0], len(arr), minfdict['nn'], args['ndf']) ## Usually used for data.all files


        # Write to BIN output file
        if args['write']:
            if len(out_arrays) > 1:
                for count, arr in enumerate(out_arrays):
                    writeoutfile = args['write'] + str(count)
                    arr_to_bin(arr, writeoutfile, dataformat)
            else:
                # arr_to_bin(arr, args['write'], dataformat)
                arr_to_bin(out_arrays[0], args['write'], dataformat)

        # To quickly plot a packing to see if it's square/cylindrical, and ordered/random
        # ## TODO: Copy this over to pack.py as well
        if args['plot']:
            for arr in out_arrays:
                x = [ arr[i] for i in range(len(arr)) if i%4 == 0 ]
                y = [ arr[i] for i in range(len(arr)) if i%4 == 1 ]
                z = [ arr[i] for i in range(len(arr)) if i%4 == 2 ]
                scatterPlot(x,y,z, s=args['scatter_size'])

if __name__ == "__main__":
    # print(__doc__)
    main()
